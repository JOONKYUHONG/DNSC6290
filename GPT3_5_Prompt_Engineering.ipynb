{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN4yS05g/2cbkjzIgLCZ1Gz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JOONKYUHONG/DNSC6290_RML/blob/main/GPT3_5_Prompt_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic setting"
      ],
      "metadata": {
        "id": "0Wa4EdZeT6wD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p__S9a7MTJpq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#Everyone needs to save the dataset in their own drive.\n",
        "drive.mount(\"/content/gdrive/\")"
      ],
      "metadata": {
        "id": "htV-38M7Tocr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "complaints = pd.read_csv('/content/gdrive/MyDrive/WF/complaints.csv')"
      ],
      "metadata": {
        "id": "bAddqHQtTsbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where 'Consumer complaint narrative' is null\n",
        "complaints = complaints.dropna(subset=['Consumer complaint narrative'])"
      ],
      "metadata": {
        "id": "O73fQeytTuMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complaints.isnull().sum()"
      ],
      "metadata": {
        "id": "4kdHORhBUdnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complaints_revised = complaints[['Company', 'Consumer complaint narrative']]"
      ],
      "metadata": {
        "id": "piHEaSyoUfMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Choose five banks for sample\n",
        "company_list = ['WELLS FARGO & COMPANY', 'BANK OF AMERICA, NATIONAL ASSOCIATION', 'CAPITAL ONE FINANCIAL CORPORATION', 'JPMORGAN CHASE $ CO.', 'CITIBANK, N.A.']\n",
        "complaints_revised = complaints_revised[complaints_revised['Company'].isin(company_list)]\n",
        "complaints_revised.head(10)"
      ],
      "metadata": {
        "id": "rkYVbvoKUm4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complaints_revised.info()"
      ],
      "metadata": {
        "id": "nrRCYaEAUqiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Specify the sample size you want - 50\n",
        "sample_size = 50\n",
        "\n",
        "# Extract the 'Consumer complaint narrative' column as a list\n",
        "narratives = complaints_revised['Consumer complaint narrative'].tolist()\n",
        "\n",
        "# Check if the number of narratives is greater than the desired sample size\n",
        "if len(narratives) > sample_size:\n",
        "    random_samples = random.sample(narratives, sample_size)\n",
        "else:\n",
        "    random_samples = narratives\n",
        "\n",
        "# Create a DataFrame from the sampled narratives\n",
        "sample = pd.DataFrame({'Consumer complaint narrative': random_samples})"
      ],
      "metadata": {
        "id": "xcfJzkN-Uvuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample.head(10)"
      ],
      "metadata": {
        "id": "yQhXXkGiU2uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample.isnull().sum()"
      ],
      "metadata": {
        "id": "yrGUQyD-VIAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0_Setup\n",
        "#### Load the API key and relevant Python libaries."
      ],
      "metadata": {
        "id": "mIUcOoutWLym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = 'sk-******************************'\n",
        "# each person has to use own api key"
      ],
      "metadata": {
        "id": "nZo_kf9aWQyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "XbuEDjgjWdkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import time\n",
        "\n",
        "openai.api_key  = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "_CyrMBx1WgbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### helper function\n",
        "we will use OpenAI's `gpt-3.5-turbo` model and the [chat completions endpoint](https://platform.openai.com/docs/guides/chat).\n",
        "\n",
        "This helper function will make it easier to use prompts and look at the generated outputs:"
      ],
      "metadata": {
        "id": "FX_17XLMWkUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "1ehsCDG8WxSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add error, retry\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0, verbose=False):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    time_start = time.time()\n",
        "    retry_count = 3\n",
        "    for i in range(0, retry_count):\n",
        "        while True:\n",
        "            try:\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=model,\n",
        "                    messages=messages,\n",
        "                    temperature=temperature, # this is the degree of randomness of the model's output\n",
        "                )\n",
        "                answer = response['choices'][0]['message']['content'].strip()\n",
        "                tokens = response.usage.total_tokens\n",
        "\n",
        "\n",
        "                time_end = time.time()\n",
        "\n",
        "                if verbose:\n",
        "                    print('prompt: %s | token: %d | %.1fsec\\nanwer : %s'%(prompt, tokens, (time_end - time_start), answer))\n",
        "                return answer\n",
        "\n",
        "            except Exception as error:\n",
        "                print(f\"API Error: {error}\")\n",
        "                print(f\"Retrying {i+1} time(s) in 4 seconds...\")\n",
        "\n",
        "                if i+1 == retry_count:\n",
        "                    return prompt, None, None\n",
        "                time.sleep(4)\n",
        "                continue"
      ],
      "metadata": {
        "id": "zAhgHe2SW41G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Please explain what ChatGPT in 20 words'\n",
        "answer = get_completion(prompt, model=\"gpt-3.5-turbo\")\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "fXtGu8urW7FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1_Guidelines for Prompting\n",
        "two prompting principles and their related tactics in order to write effective prompts for large language models."
      ],
      "metadata": {
        "id": "nMTKyjGQXFVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting Principles\n",
        "- **Principle 1: Write clear and specific instructions**  \n",
        "- **Principle 2: Give the model time to “think”**\n",
        "\n",
        "### Tactics\n",
        "\n",
        "#### Tactic 1: Use delimiters to clearly indicate distinct parts of the input,  \n",
        "- Delimiters can be anything like: ```, \"\"\", < >, `<tag> </tag>`, `:`,  "
      ],
      "metadata": {
        "id": "LDLu29LnXgpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = f\"\"\"\n",
        "You should express what you want a model to do by \\\n",
        "providing instructions that are as clear and \\\n",
        "specific as you can possibly make them. \\\n",
        "This will guide the model towards the desired output, \\\n",
        "and reduce the chances of receiving irrelevant \\\n",
        "or incorrect responses. Don't confuse writing a \\\n",
        "clear prompt with writing a short prompt. \\\n",
        "In many cases, longer prompts provide more clarity \\\n",
        "and context for the model, which can lead to \\\n",
        "more detailed and relevant outputs.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "Summarize the text delimited by triple backticks \\\n",
        "into a single sentence.\n",
        "```{text}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "tl0KkqFOXsd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principle 2: Give the model time to “think”\n",
        "\n",
        "#### Tactic 1: Specify the steps required to complete a task"
      ],
      "metadata": {
        "id": "oveo_2K-ljiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = f\"\"\"\n",
        "In a charming village, siblings Jack and Jill set out on \\\n",
        "a quest to fetch water from a hilltop \\\n",
        "well. As they climbed, singing joyfully, misfortune \\\n",
        "struck—Jack tripped on a stone and tumbled \\\n",
        "down the hill, with Jill following suit. \\\n",
        "Though slightly battered, the pair returned home to \\\n",
        "comforting embraces. Despite the mishap, \\\n",
        "their adventurous spirits remained undimmed, and they \\\n",
        "continued exploring with delight.\n",
        "\"\"\"\n",
        "# example 1\n",
        "prompt_1 = f\"\"\"\n",
        "Perform the following actions:\n",
        "1 - Summarize the following text delimited by triple \\\n",
        "backticks with 1 sentence.\n",
        "2 - Translate the summary into French.\n",
        "3 - List each name in the French summary.\n",
        "4 - Output a json object that contains the following \\\n",
        "keys: french_summary, num_names.\n",
        "\n",
        "Separate your answers with line breaks.\n",
        "\n",
        "Text:\n",
        "```{text}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt_1)\n",
        "print(\"Completion for prompt 1:\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "KIiL6JxzlhLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synthesize Complaints"
      ],
      "metadata": {
        "id": "iSLfsPQJesxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* The initial exploratory analysis by the GW project team identified three types of complaints:\n",
        " * Complaints that are generally well written - DESIRABLE\n",
        " * Complaints with very poor grammar and punctuation - PROBLEMATIC\n",
        " * Complaints with frivolous, unrelated information - PROBLEMATIC\n",
        "\n",
        "\n",
        "\n",
        "Based on this, we had this idea: Have the team focus on just the invariance and robustness aspects related to these “problematic” complaints and focus on questions like “Can the right LLM with the right prompt ‘clean up’ these problematic complaints?” In other words, can an LLM correct the poor grammar and punctuation without changing the meaning (invariance) and can an LLM identify the frivolous content of a complaint (robustness)?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kQC9uOIkqf97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample Complaints - Lets see how it looks like"
      ],
      "metadata": {
        "id": "sCgkBXNFwF55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_samples[:5]"
      ],
      "metadata": {
        "id": "6nw4DdEewKG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt for Robustness Testing"
      ],
      "metadata": {
        "id": "nx9daAaWwTZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(random_samples[0:5])):\n",
        "    prompt = f\"\"\"\n",
        "    Your task is to examine if customer \\\n",
        "    complaints from banks are well written without any issue.\n",
        "\n",
        "    Determine if complaints are well-written (considered desirable), \\\n",
        "    exhibit severe grammar or punctuation issues (considered problematic), \\\n",
        "    or contain irrelevant, frivolous information (also considered problematic). \\\n",
        "\n",
        "    Make your response as short as possible.\n",
        "\n",
        "    Complaints: ```{random_samples[i]}```\n",
        "    \"\"\"\n",
        "\n",
        "    response = get_completion(prompt)\n",
        "    print(i, response, \"\\n\")"
      ],
      "metadata": {
        "id": "KcF5JGB9qiyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see some complaints have issues related to grammar. Let's see if LLM can change complaints look better."
      ],
      "metadata": {
        "id": "Fb7uxJ9brMaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt for Equal Complaint Synthesis (MFT)"
      ],
      "metadata": {
        "id": "mepAmQm7nL8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(random_samples[0:5])):\n",
        "    prompt = f\"\"\"\n",
        "    Your task is to generate a short synthesized customer \\\n",
        "    complaints from banks.\n",
        "\n",
        "    Generate synthesized complaints by rearranging words and sentence structures \\\n",
        "    while maintaining the same meaning and intensity as the original \\\n",
        "    complaints in at most 50 words.\n",
        "\n",
        "    Focus on maintaining equality in the sentiment. \\\n",
        "\n",
        "    Complaints: ```{random_samples[i]}```\n",
        "    \"\"\"\n",
        "\n",
        "    response = get_completion(prompt)\n",
        "    print(i, response, \"\\n\")"
      ],
      "metadata": {
        "id": "Svzj9XHCnRQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt for Invariance Complaint Synthesis"
      ],
      "metadata": {
        "id": "fGIvxdxloEvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(random_samples[0:5])):\n",
        "    prompt = f\"\"\"\n",
        "    Your task is to generate a short synthesized customer \\\n",
        "    complaints from banks.\n",
        "\n",
        "    Create synthetic complaints that retain the core message \\\n",
        "    and sentiment of the original complaints but change some words \\\n",
        "    or phrases while ensuring there is no change in the overall meaning \\\n",
        "    in at most 50 words.\n",
        "\n",
        "    Complaints: ```{random_samples[i]}```\n",
        "    \"\"\"\n",
        "\n",
        "    response = get_completion(prompt)\n",
        "    print(i, response, \"\\n\")"
      ],
      "metadata": {
        "id": "FmOBeIGOf8zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt for Harshness Modification (Direction):"
      ],
      "metadata": {
        "id": "IKzfY299oTQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(random_samples[0:5])):\n",
        "    prompt = f\"\"\"\n",
        "    Your task is to generate a short synthesized customer \\\n",
        "    complaints from banks.\n",
        "\n",
        "    Develop a method to make complaints more harsh or less harsh \\\n",
        "    while keeping the underlying issue intact in at most 50 words\\\n",
        "\n",
        "    Use a range of intensity levels (Less Harsh or More harsh)) to ensure diversity.\n",
        "\n",
        "    Complaints: ```{random_samples[i]}```\n",
        "    \"\"\"\n",
        "\n",
        "    response = get_completion(prompt)\n",
        "    print(i, response, \"\\n\")"
      ],
      "metadata": {
        "id": "z2xMLKxxX4cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wL4lpLXbo_ig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}